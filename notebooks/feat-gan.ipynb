{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from src.models.feat_vae import FeatVAE\n",
    "from src.models.classifier import FeatClassifier\n",
    "from src.dataloaders.load_data import load_data\n",
    "from firelab.config import Config\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "\n",
    "config = Config.load('../configs/lgm_vae.yml', frozen=False)\n",
    "config.all.hp.model.set('pretrained', True)\n",
    "config.all.hp.model.set('num_classes', NUM_CLASSES)\n",
    "config.all.hp.model.resnet_type = 'mnist_1d'\n",
    "# config = config.overwrite(Config({'all': {'hp': {'model': {'learn_prior' : True}}}}))\n",
    "config.all.hp.model.learn_prior = True\n",
    "base_conf = Config.load('../configs/base.yml')\n",
    "base_conf = base_conf.overwrite(Config({'cub_embeddings': {'data': {'dir': \"../data/CUB_EMBEDDINGS\"}}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as T\n",
    "\n",
    "\n",
    "train_ds = MNIST('../data/mnist', train=True, transform=T.ToTensor())\n",
    "test_ds = MNIST('../data/mnist', train=False, transform=T.ToTensor())\n",
    "\n",
    "# train_imgs = np.array([d[0] for d in train_ds])\n",
    "# train_imgs /= np.abs(train_imgs).max(axis=0, keepdims=True)\n",
    "# train_imgs -= train_imgs.mean(axis=0, keepdims=True)\n",
    "\n",
    "# test_imgs = np.array([d[0] for d in test_ds])\n",
    "# test_imgs /= np.abs(train_imgs).max(axis=0, keepdims=True)\n",
    "# test_imgs -= train_imgs.mean(axis=0, keepdims=True)\n",
    "\n",
    "# train_ds = [(train_imgs[i], train_ds[i][1]) for i in range(len(train_ds))]\n",
    "# test_ds = [(test_imgs[i], test_ds[i][1]) for i in range(len(test_ds))]\n",
    "\n",
    "# train_ds = [(np.tanh(x), y) for x, y in train_ds]\n",
    "# test_ds = [(np.tanh(x), y) for x, y in test_ds]\n",
    "\n",
    "train_dataloader = DataLoader(train_ds, batch_size=256, shuffle=True, collate_fn=lambda b: list(zip(*b)))\n",
    "test_dataloader = DataLoader(test_ds, batch_size=256, shuffle=False, collate_fn=lambda b: list(zip(*b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:14<00:00,  7.49s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def compute_prototype(ds, label: int):\n",
    "    return np.mean([x.numpy() for x, y in ds if y == label], axis=0)\n",
    "\n",
    "prototypes = np.array([compute_prototype(train_ds, y) for y in tqdm(range(NUM_CLASSES))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_vae(vae_model, dataloader):\n",
    "    rec_losses = []\n",
    "    kl_losses = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = torch.stack([img.flatten() for img in x]).to(DEVICE)\n",
    "            y = torch.tensor(y).to(DEVICE)\n",
    "\n",
    "            z_mean, z_logvar = vae_model.encoder(x, y)\n",
    "            z_prior_mean, z_prior_logvar = vae_model.prior(y)\n",
    "            z = vae_model.sample(z_mean, z_logvar)\n",
    "            x_rec = vae_model.decoder(z, y)\n",
    "\n",
    "            rec_loss = F.mse_loss(x_rec, x, reduction='none').mean(dim=1)\n",
    "            kl_loss = compute_kl_loss(z_mean, z_logvar, z_prior_mean, z_prior_logvar, reduction=None)\n",
    "\n",
    "            rec_losses.extend(rec_loss.cpu().tolist())\n",
    "            kl_losses.extend(kl_loss.cpu().tolist())\n",
    "\n",
    "    return np.mean(rec_losses), np.mean(kl_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = config.overwrite(Config({'all': {'hp': {'model': {'z_dim' : 512}}}}))\n",
    "vae = FeatVAE(config.all.hp.model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch #000 T] rec:  0.0535. KL:  0.0035. P:  0.0000\n",
      "[Epoch #000] Val rec loss  :  0.0548. Val KL loss  :  0.0034\n",
      "[Epoch #001 T] rec:  0.0509. KL:  0.0019. P:  0.0000\n",
      "[Epoch #001] Val rec loss  :  0.0546. Val KL loss  :  0.0020\n",
      "[Epoch #002 T] rec:  0.0528. KL:  0.0014. P:  0.0000\n",
      "[Epoch #002] Val rec loss  :  0.0542. Val KL loss  :  0.0014\n",
      "[Epoch #003 T] rec:  0.0544. KL:  0.0012. P:  0.0000\n",
      "[Epoch #003] Val rec loss  :  0.0538. Val KL loss  :  0.0011\n",
      "[Epoch #004 T] rec:  0.0536. KL:  0.0009. P:  0.0000\n",
      "[Epoch #004] Val rec loss  :  0.0536. Val KL loss  :  0.0009\n",
      "[Epoch #005 T] rec:  0.0512. KL:  0.0007. P:  0.0000\n",
      "[Epoch #005] Val rec loss  :  0.0535. Val KL loss  :  0.0008\n",
      "[Epoch #006 T] rec:  0.0527. KL:  0.0006. P:  0.0000\n",
      "[Epoch #006] Val rec loss  :  0.0534. Val KL loss  :  0.0006\n",
      "[Epoch #007 T] rec:  0.0528. KL:  0.0005. P:  0.0000\n",
      "[Epoch #007] Val rec loss  :  0.0533. Val KL loss  :  0.0006\n",
      "[Epoch #008 T] rec:  0.0538. KL:  0.0005. P:  0.0000\n",
      "[Epoch #008] Val rec loss  :  0.0533. Val KL loss  :  0.0005\n",
      "[Epoch #009 T] rec:  0.0511. KL:  0.0004. P:  0.0000\n",
      "[Epoch #009] Val rec loss  :  0.0533. Val KL loss  :  0.0004\n"
     ]
    }
   ],
   "source": [
    "from src.utils.losses import compute_kld_between_diagonal_gaussians as compute_kl_loss\n",
    "\n",
    "optim = torch.optim.Adam(vae.parameters(), lr=1e-4)\n",
    "max_num_epochs = 10\n",
    "beta = 1.\n",
    "proto_loss_coef = 100.\n",
    "train_rec_loss_history = []\n",
    "train_kl_loss_history = []\n",
    "\n",
    "\n",
    "for epoch in range(max_num_epochs):\n",
    "    for x, y in train_dataloader:\n",
    "        x = torch.stack([img.flatten() for img in x]).to(DEVICE)\n",
    "        y = torch.tensor(y).to(DEVICE)\n",
    "        \n",
    "        z_mean, z_logvar = vae.encoder(x, y)\n",
    "        z_prior_mean, z_prior_logvar = vae.prior(y)\n",
    "        z = vae.sample(z_mean, z_logvar, noise_level=epoch/max_num_epochs)\n",
    "        x_rec = vae.decoder(z, y)\n",
    "        \n",
    "        rec_loss = F.mse_loss(x_rec, x)\n",
    "        kl_loss = compute_kl_loss(z_mean, z_logvar, z_prior_mean, z_prior_logvar)\n",
    "        \n",
    "        #proto_loss = F.mse_loss(x_rec, torch.from_numpy(prototypes[y]))\n",
    "        proto_loss = torch.zeros(1)\n",
    "\n",
    "        total_loss = rec_loss + beta * kl_loss + proto_loss_coef * proto_loss\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    val_rec_loss, val_kl_loss = validate_vae(vae, test_dataloader)\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        print(f'[Epoch #{epoch:03d} T] rec: {rec_loss.item(): 0.4f}. KL: {kl_loss.item(): 0.4f}. P: {proto_loss.item(): 0.4f}')\n",
    "        print(f'[Epoch #{epoch:03d}] Val rec loss  : {val_rec_loss.item(): 0.4f}. Val KL loss  : {val_kl_loss.item(): 0.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from src.models.feat_gan import FeatGenerator, FeatDiscriminator\n",
    "\n",
    "gan_config = Config.load('../configs/lgm.yml', frozen=False)\n",
    "gan_config.all.hp.model.set('pretrained', True)\n",
    "gan_config.all.hp.model.set('num_classes', 200)\n",
    "gan_config.all.hp.model.use_attrs_in_gen = False\n",
    "gan_config.all.hp.model.use_attrs_in_discr = False\n",
    "gan_config.all.hp.model.cls_hid_dim = 1024\n",
    "# gan_config.all.hp.model.hid_dim = 2048\n",
    "gan_config.all.hp.model.share_body_in_discr = True\n",
    "\n",
    "\n",
    "def compute_proto_loss(x, prototypes) -> torch.Tensor:\n",
    "    assert len(x) % len(prototypes) == 0\n",
    "    \n",
    "    prototypes = torch.from_numpy(prototypes) \\\n",
    "        .repeat(len(x) // len(prototypes), 1) \\\n",
    "        .to(DEVICE)\n",
    "    \n",
    "    return F.mse_loss(x, prototypes)\n",
    "\n",
    "\n",
    "def generator_step(gen, discr, optim_gen, x, prototypes, cls_gen_coef:float=1., proto_loss_coef:float=1., gen_batch_size = 1000):\n",
    "    z = gen.sample_noise(gen_batch_size).to(DEVICE)\n",
    "    y = torch.arange(NUM_CLASSES).repeat(gen_batch_size // NUM_CLASSES).to(DEVICE)\n",
    "    x_fake = gen(z, y)\n",
    "    adv_logits, cls_logits = discr(x_fake)\n",
    "    adv_loss = -adv_logits.mean()\n",
    "    cls_loss = F.cross_entropy(cls_logits, y)\n",
    "    cls_acc = (cls_logits.argmax(dim=1) == y).float().mean().detach().cpu()\n",
    "    #proto_loss = proto_loss_coef * F.mse_loss(x_fake, torch.from_numpy(prototypes[y]).to(DEVICE))\n",
    "    proto_loss = compute_proto_loss(x_fake, prototypes)\n",
    "    \n",
    "    total_loss = adv_loss + cls_gen_coef * cls_loss + proto_loss_coef + proto_loss\n",
    "    \n",
    "    optim_gen.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optim_gen.step()\n",
    "    \n",
    "    return adv_loss, cls_acc, proto_loss\n",
    "    \n",
    "    \n",
    "def discriminator_step(gen, discr, optim_discr, x, y, gp_coef:float=10.,\n",
    "                       cls_discr_coef:float=1., cls_adv_loss_coef:float=1.):\n",
    "    with torch.no_grad():\n",
    "        z = gen.sample_noise(y.size(0)).to(DEVICE)\n",
    "        x_fake = gen(z, y) # TODO: try picking y randomly\n",
    "\n",
    "    adv_logits_on_real, cls_logits_on_real = discr(x)\n",
    "    adv_logits_on_fake, cls_logits_on_fake = discr(x_fake)\n",
    "\n",
    "    adv_loss = -adv_logits_on_real.mean() + adv_logits_on_fake.mean()\n",
    "    grad_penalty = compute_gradient_penalty(discr.run_adv_head, x, x_fake)\n",
    "    \n",
    "    cls_loss = F.cross_entropy(cls_logits_on_real, y)\n",
    "    cls_acc = (cls_logits_on_real.argmax(dim=1) == y).float().mean().detach().cpu()\n",
    "    \n",
    "    # Cross-entropy with uniform dist\n",
    "    #cls_adv_loss = (cls_logits_on_fake.logsumexp(dim=1) - cls_logits_on_fake.sum(dim=1) / NUM_CLASSES).mean()\n",
    "    cls_adv_loss = F.kl_div(cls_logits_on_fake.log_softmax(dim=1), torch.ones_like(cls_logits_on_fake) / cls_logits_on_fake.shape[1])\n",
    "    #cls_adv_loss = -F.cross_entropy(cls_logits_on_fake, y)\n",
    "    \n",
    "    #if cls_adv_loss.abs() > 10:\n",
    "    #    cls_adv_loss = (cls_adv_loss / cls_adv_loss.abs().detach()) * 10\n",
    "\n",
    "    total_loss = adv_loss + gp_coef * grad_penalty + cls_discr_coef * cls_loss + cls_adv_loss_coef * cls_adv_loss\n",
    "    \n",
    "    optim_discr.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optim_discr.step()\n",
    "\n",
    "    return adv_loss, grad_penalty, cls_acc, cls_adv_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = FeatGenerator(gan_config.all.hp.model).to(DEVICE)\n",
    "discr = FeatDiscriminator(gan_config.all.hp.model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/skoroki/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Using a target size (torch.Size([1000, 784])) that is different to the input size (torch.Size([1000, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (784) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-2318d539759b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum_iters_done\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_discr_steps\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             gen_adv_loss, gen_cls_acc, proto_loss = generator_step(\n\u001b[0;32m---> 37\u001b[0;31m                 gen, discr, optim_gen, x, prototypes)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mgen_adv_losses_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_adv_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-d0e341d4c00d>\u001b[0m in \u001b[0;36mgenerator_step\u001b[0;34m(gen, discr, optim_gen, x, prototypes, cls_gen_coef, proto_loss_coef, gen_batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcls_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcls_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m#proto_loss = proto_loss_coef * F.mse_loss(x_fake, torch.from_numpy(prototypes[y]).to(DEVICE))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mproto_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_proto_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madv_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcls_gen_coef\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcls_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproto_loss_coef\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mproto_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-d0e341d4c00d>\u001b[0m in \u001b[0;36mcompute_proto_loss\u001b[0;34m(x, prototypes)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprototypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2201\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2202\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2203\u001b[0;31m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2204\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 [0, 1, 2]])\n\u001b[1;32m     51\u001b[0m     \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_VariableFunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (784) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from src.utils.losses import compute_gradient_penalty\n",
    "\n",
    "optim_gen = torch.optim.Adam(gen.parameters())\n",
    "optim_discr = torch.optim.Adam(discr.parameters())\n",
    "max_num_epochs = 50\n",
    "num_discr_steps = 5\n",
    "cls_discr_coef = 1.\n",
    "cls_adv_loss_coef = 10.\n",
    "\n",
    "num_iters_done = 0\n",
    "gen_adv_losses_hist = []\n",
    "gen_cls_acc_hist = []\n",
    "discr_adv_losses_hist = []\n",
    "gp_hist = []\n",
    "discr_cls_acc_hist = []\n",
    "cls_adv_loss_hist = []\n",
    "proto_loss_hist = []\n",
    "plot_every_epoch = 1\n",
    "\n",
    "\n",
    "for epoch in range(max_num_epochs):\n",
    "    for x, y in train_dataloader:\n",
    "        x = x = torch.stack([img.flatten() for img in x]).to(DEVICE)\n",
    "        y = torch.tensor(y).to(DEVICE)\n",
    "        \n",
    "        if num_iters_done % (num_discr_steps + 1) == 0:\n",
    "            gen_adv_loss, gen_cls_acc, proto_loss = generator_step(\n",
    "                gen, discr, optim_gen, x, prototypes)\n",
    "            \n",
    "            gen_adv_losses_hist.append(gen_adv_loss.detach().cpu().item())\n",
    "            gen_cls_acc_hist.append(gen_cls_acc.item())\n",
    "            proto_loss_hist.append(proto_loss.detach().cpu().item())\n",
    "        else:\n",
    "            discr_adv_loss, grad_penalty, discr_cls_acc, cls_adv_loss = discriminator_step(\n",
    "                gen, discr, optim_discr, x, y, cls_discr_coef=cls_discr_coef, cls_adv_loss_coef=cls_adv_loss_coef)\n",
    "            \n",
    "            discr_adv_losses_hist.append(discr_adv_loss.detach().cpu().item())\n",
    "            gp_hist.append(grad_penalty.detach().cpu().item())\n",
    "            discr_cls_acc_hist.append(discr_cls_acc.item())\n",
    "            cls_adv_loss_hist.append(cls_adv_loss.detach().cpu().item())\n",
    "            \n",
    "        num_iters_done += 1\n",
    "            \n",
    "    if epoch % plot_every_epoch == 0:\n",
    "        clear_output(True)\n",
    "        plt.figure(figsize=[20,10])\n",
    "\n",
    "        plt.subplot(241)\n",
    "        plt.title(f\"Generator loss (epoch #{epoch})\")\n",
    "        plt.plot(gen_adv_losses_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(gen_adv_losses_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(242)\n",
    "        plt.title(f\"Generator cls acc (epoch #{epoch})\")\n",
    "        plt.plot(gen_cls_acc_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(gen_cls_acc_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(243)\n",
    "        plt.title(f\"Discriminator loss (epoch #{epoch})\")\n",
    "        plt.plot(discr_adv_losses_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(discr_adv_losses_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(244)\n",
    "        plt.title(f\"Discriminator GP (epoch #{epoch})\")\n",
    "        plt.plot(gp_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(gp_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(245)\n",
    "        plt.title(f\"Discriminator cls acc (epoch #{epoch})\")\n",
    "        plt.plot(discr_cls_acc_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(discr_cls_acc_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(246)\n",
    "        plt.title(f\"Cls adv loss (epoch #{epoch})\")\n",
    "        plt.plot(cls_adv_loss_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(cls_adv_loss_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "        \n",
    "        plt.subplot(247)\n",
    "        plt.title(f\"Proto loss (epoch #{epoch})\")\n",
    "        plt.plot(proto_loss_hist, color='#33ACFF')\n",
    "        plt.plot(pd.DataFrame(proto_loss_hist).ewm(span=100).mean(), color='#0000FF')\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_clf(clf_model, dataloader):\n",
    "    losses = []\n",
    "    accs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            #x = torch.tensor(x).to(DEVICE)\n",
    "            x = torch.stack([img.flatten() for img in x]).to(DEVICE)\n",
    "            y = torch.tensor(y).to(DEVICE)\n",
    "\n",
    "            logits = clf_model(x)\n",
    "            loss = F.cross_entropy(logits, y, reduction='none').cpu().tolist()\n",
    "            acc = (logits.argmax(dim=1) == y).float().cpu().tolist()\n",
    "            \n",
    "            losses.extend(loss)\n",
    "            accs.extend(acc)\n",
    "        \n",
    "    return np.mean(losses), np.mean(accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step #0000] Train loss:  5.3059. Train acc:  0.0039\n",
      "[Step #0000] Val loss  :  5.2204. Val acc  :  0.0480\n",
      "[Step #0010] Train loss:  5.2742. Train acc:  0.0117\n",
      "[Step #0010] Val loss  :  4.5818. Val acc  :  0.3352\n",
      "[Step #0020] Train loss:  5.2322. Train acc:  0.0156\n",
      "[Step #0020] Val loss  :  3.6478. Val acc  :  0.5769\n",
      "[Step #0030] Train loss:  5.0826. Train acc:  0.0586\n",
      "[Step #0030] Val loss  :  2.7897. Val acc  :  0.7145\n",
      "[Step #0040] Train loss:  5.1111. Train acc:  0.0273\n",
      "[Step #0040] Val loss  :  2.0425. Val acc  :  0.6287\n",
      "[Step #0050] Train loss:  5.0191. Train acc:  0.0625\n",
      "[Step #0050] Val loss  :  1.5602. Val acc  :  0.7218\n",
      "[Step #0060] Train loss:  4.7849. Train acc:  0.0859\n",
      "[Step #0060] Val loss  :  1.1373. Val acc  :  0.7724\n",
      "[Step #0070] Train loss:  4.7333. Train acc:  0.1914\n",
      "[Step #0070] Val loss  :  0.9992. Val acc  :  0.7738\n",
      "[Step #0080] Train loss:  4.5971. Train acc:  0.2148\n",
      "[Step #0080] Val loss  :  1.0080. Val acc  :  0.7679\n",
      "[Step #0090] Train loss:  4.4636. Train acc:  0.2734\n",
      "[Step #0090] Val loss  :  1.1076. Val acc  :  0.7628\n",
      "[Step #0100] Train loss:  4.3499. Train acc:  0.3594\n",
      "[Step #0100] Val loss  :  1.1600. Val acc  :  0.7551\n",
      "[Step #0110] Train loss:  4.0324. Train acc:  0.4297\n",
      "[Step #0110] Val loss  :  1.2384. Val acc  :  0.7452\n",
      "[Step #0120] Train loss:  4.0112. Train acc:  0.4609\n",
      "[Step #0120] Val loss  :  1.4170. Val acc  :  0.7206\n",
      "[Step #0130] Train loss:  3.7802. Train acc:  0.5898\n",
      "[Step #0130] Val loss  :  1.6040. Val acc  :  0.6899\n",
      "[Step #0140] Train loss:  3.5874. Train acc:  0.6172\n",
      "[Step #0140] Val loss  :  1.7060. Val acc  :  0.6866\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-614321a660fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "clf = FeatClassifier(config.all.hp.model).to(DEVICE)\n",
    "optim = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "max_num_steps = 1000\n",
    "batch_size = 256\n",
    "\n",
    "for step in range(max_num_steps):\n",
    "    with torch.no_grad():\n",
    "        y = random.choices(range(200), k=batch_size)\n",
    "        y = torch.tensor(y).to(DEVICE)\n",
    "        #x = gen.sample(y)\n",
    "        x = vae.generate(y)\n",
    "        \n",
    "    logits = clf(x)\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    acc = (logits.argmax(dim=1) == y).float().mean().cpu().detach()\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        val_loss, val_acc = validate_clf(clf, test_dataloader)\n",
    "\n",
    "        print(f'[Step #{step:04d}] Train loss: {loss.item(): 0.4f}. Train acc: {acc.item(): 0.4f}')\n",
    "        print(f'[Step #{step:04d}] Val loss  : {val_loss.item(): 0.4f}. Val acc  : {val_acc.item(): 0.4f}')\n",
    "        \n",
    "print('Train loss/acc:', validate_clf(clf, train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0205,  0.0907, -0.0357,  ...,  0.0055, -0.0262,  0.0615],\n",
       "        [-0.0040,  0.0930, -0.0491,  ...,  0.0789,  0.0038,  0.0873],\n",
       "        [-0.0053,  0.0885, -0.0278,  ...,  0.0084,  0.0053,  0.1047],\n",
       "        ...,\n",
       "        [ 0.0091,  0.1011, -0.0184,  ...,  0.0557,  0.0033,  0.0751],\n",
       "        [ 0.0383,  0.1043, -0.0466,  ...,  0.0814, -0.0044,  0.0737],\n",
       "        [ 0.0317,  0.0740, -0.0656,  ...,  0.0203, -0.0071,  0.1984]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.prior.model(vae.prior.embedder(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch #0000] Train loss:  5.2211. Train acc:  0.0283\n",
      "[Epoch #0000] Val loss  :  5.1892. Val acc  :  0.0307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-50832135d276>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clf = FeatClassifier(config.all.hp.model).to(DEVICE)\n",
    "optim = torch.optim.Adam(clf.parameters(), lr=1e-3)\n",
    "max_num_epochs = 100\n",
    "\n",
    "for epoch in range(max_num_epochs):\n",
    "    for x, y in train_dataloader:\n",
    "        x = torch.tensor(x).to(DEVICE)\n",
    "        y = torch.tensor(y).to(DEVICE)\n",
    "\n",
    "        logits = clf(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = (logits.argmax(dim=1) == y).float().mean().cpu().detach()\n",
    "\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        val_loss, val_acc = validate_clf(clf, test_dataloader)\n",
    "        \n",
    "        print(f'[Epoch #{epoch:04d}] Train loss: {loss.item(): 0.4f}. Train acc: {acc.item(): 0.4f}')\n",
    "        print(f'[Epoch #{epoch:04d}] Val loss  : {val_loss.item(): 0.4f}. Val acc  : {val_acc.item(): 0.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
