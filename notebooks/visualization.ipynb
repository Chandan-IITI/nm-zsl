{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "exps_dir = '../experiments'\n",
    "exps = {\n",
    "    'agem': [f'agem_cub-{i:05d}' for i in range(2, 12)],\n",
    "    'basic': [f'basic_cub-{i:05d}' for i in range(2, 12)],\n",
    "    'ewc': [f'ewc_cub-{i:05d}' for i in range(1, 11)],\n",
    "    'mas': [f'mas_cub-{i:05d}' for i in range(1, 11)]\n",
    "}\n",
    "accs_histories = {exp: np.array([np.load(f'{exps_dir}/{p}/custom_data/accs_history.npy') for p in exps[exp]]) for exp in exps}\n",
    "zst_accs = {exp: np.array([np.load(f'{exps_dir}/{p}/custom_data/zst_accs.npy') for p in exps[exp]]) for exp in exps}\n",
    "lca_scores = {exp: np.array([np.load(f'{exps_dir}/{p}/custom_data/test_acc_batch_histories.npy') for p in exps[exp]]) for exp in exps}\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.title('ZST accuracy on CUB')\n",
    "\n",
    "# # plt.plot(np.arange(1, 21), agem_scores.mean(axis=0), color='red', marker='^', label='A-GEM old (5 runs)')\n",
    "# # plt.fill_between(np.arange(1, 21),\n",
    "# #          agem_scores.mean(axis=0) + agem_scores.std(axis=0),\n",
    "# #          agem_scores.mean(axis=0) - agem_scores.std(axis=0),\n",
    "# #          color='red', alpha=0.05)\n",
    "\n",
    "# # plt.plot(np.arange(1, 21), basic_scores.mean(axis=0), color='blue', label='Baseline (averaged across 5 runs)')\n",
    "# # plt.fill_between(np.arange(1, 21),\n",
    "# #          basic_scores.mean(axis=0) + basic_scores.std(axis=0),\n",
    "# #          basic_scores.mean(axis=0) - basic_scores.std(axis=0),\n",
    "# #          color='blue', alpha=0.05)\n",
    "\n",
    "plt.plot(np.arange(1, 21), zst_accs['basic'].mean(axis=0), label='Basic')\n",
    "plt.plot(np.arange(1, 21), zst_accs['agem'].mean(axis=0), label='A-GEM')\n",
    "plt.plot(np.arange(1, 21), zst_accs['ewc'].mean(axis=0), label='EWC')\n",
    "plt.plot(np.arange(1, 21), zst_accs['mas'].mean(axis=0), label='MAS')\n",
    "\n",
    "plt.xlabel('Task ID')\n",
    "plt.ylabel('ZST accuracy for a task')\n",
    "plt.xticks(np.arange(1, 21))\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average [LCA10] score for basic: 0.58790\n",
      "Average [LCA10] score for ewc: 0.58936\n",
      "Average [LCA10] score for mas: 0.59646\n",
      "Average [LCA10] score for agem: 0.60620\n"
     ]
    }
   ],
   "source": [
    "import sys; sys.path.append('..')\n",
    "from src.utils.metrics import compute_average_accuracy, compute_forgetting_measure, compute_learning_curve_area\n",
    "\n",
    "for key in ['basic', 'ewc', 'mas', 'agem']:\n",
    "    print(f'Average [LCA10] score for {key}: {np.mean([compute_learning_curve_area(ss) for ss in lca_scores[key]]):.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average [Average Accuracy] score for basic: 68.838\n",
      "Average [Average Accuracy] score for ewc: 69.455\n",
      "Average [Average Accuracy] score for mas: 70.674\n",
      "Average [Average Accuracy] score for agem: 72.916\n"
     ]
    }
   ],
   "source": [
    "for key in ['basic', 'ewc', 'mas', 'agem']:\n",
    "    print(f'Average [Average Accuracy] score for {key}: {100 * np.mean([compute_average_accuracy(ss) for ss in accs_histories[key]]):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average [Forgetting Measure] score for basic: 0.11280\n",
      "Average [Forgetting Measure] score for ewc: 0.10755\n",
      "Average [Forgetting Measure] score for mas: 0.09701\n",
      "Average [Forgetting Measure] score for agem: 0.07884\n"
     ]
    }
   ],
   "source": [
    "for key in ['basic', 'ewc', 'mas', 'agem']:\n",
    "    print(f'Average [Forgetting Measure] score for {key}: {np.mean([compute_forgetting_measure(ss) for ss in accs_histories[key]]):.5f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
