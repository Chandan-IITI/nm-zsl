{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== Encoder ==============\n",
      "Sequential(\n",
      "  (0): Conv2d(3, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(12, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(24, 48, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      ")\n",
      "============== Decoder ==============\n",
      "Sequential(\n",
      "  (0): ConvTranspose2d(48, 24, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): ConvTranspose2d(24, 12, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): ConvTranspose2d(12, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "  (5): Sigmoid()\n",
      ")\n",
      "\n",
      "Model moved to GPU in order to speed up training.\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,  2000] loss: 0.579\n",
      "[2,  2000] loss: 0.559\n",
      "[3,  2000] loss: 0.556\n",
      "[4,  2000] loss: 0.555\n",
      "[5,  2000] loss: 0.554\n",
      "[6,  2000] loss: 0.554\n",
      "[7,  2000] loss: 0.553\n",
      "[8,  2000] loss: 0.553\n",
      "[9,  2000] loss: 0.553\n",
      "[10,  2000] loss: 0.551\n",
      "[11,  2000] loss: 0.551\n",
      "[12,  2000] loss: 0.551\n",
      "[13,  2000] loss: 0.551\n",
      "[14,  2000] loss: 0.551\n",
      "[15,  2000] loss: 0.550\n",
      "[16,  2000] loss: 0.551\n",
      "[17,  2000] loss: 0.551\n",
      "[18,  2000] loss: 0.551\n",
      "[19,  2000] loss: 0.550\n",
      "[20,  2000] loss: 0.551\n",
      "[21,  2000] loss: 0.551\n",
      "[22,  2000] loss: 0.550\n",
      "[23,  2000] loss: 0.550\n",
      "[24,  2000] loss: 0.550\n",
      "[25,  2000] loss: 0.550\n",
      "[26,  2000] loss: 0.551\n",
      "[27,  2000] loss: 0.550\n",
      "[28,  2000] loss: 0.550\n",
      "[29,  2000] loss: 0.550\n",
      "[30,  2000] loss: 0.550\n",
      "[31,  2000] loss: 0.550\n",
      "[32,  2000] loss: 0.550\n",
      "[33,  2000] loss: 0.550\n",
      "[34,  2000] loss: 0.550\n",
      "[35,  2000] loss: 0.550\n",
      "[36,  2000] loss: 0.550\n",
      "[37,  2000] loss: 0.550\n",
      "[38,  2000] loss: 0.549\n",
      "[39,  2000] loss: 0.550\n",
      "[40,  2000] loss: 0.549\n",
      "[41,  2000] loss: 0.549\n",
      "[42,  2000] loss: 0.549\n",
      "[43,  2000] loss: 0.550\n",
      "[44,  2000] loss: 0.550\n",
      "[45,  2000] loss: 0.550\n",
      "[46,  2000] loss: 0.549\n",
      "[47,  2000] loss: 0.549\n",
      "[48,  2000] loss: 0.550\n",
      "[49,  2000] loss: 0.549\n",
      "[50,  2000] loss: 0.549\n",
      "[51,  2000] loss: 0.550\n",
      "[52,  2000] loss: 0.550\n",
      "[53,  2000] loss: 0.550\n",
      "[54,  2000] loss: 0.550\n",
      "[55,  2000] loss: 0.549\n",
      "[56,  2000] loss: 0.550\n",
      "[57,  2000] loss: 0.549\n",
      "[58,  2000] loss: 0.549\n",
      "[59,  2000] loss: 0.549\n",
      "[60,  2000] loss: 0.549\n",
      "[61,  2000] loss: 0.549\n",
      "[62,  2000] loss: 0.549\n",
      "[63,  2000] loss: 0.550\n",
      "[64,  2000] loss: 0.549\n",
      "[65,  2000] loss: 0.550\n",
      "[66,  2000] loss: 0.550\n",
      "[67,  2000] loss: 0.550\n",
      "[68,  2000] loss: 0.549\n",
      "[69,  2000] loss: 0.549\n",
      "[70,  2000] loss: 0.549\n",
      "[71,  2000] loss: 0.549\n",
      "[72,  2000] loss: 0.550\n",
      "[73,  2000] loss: 0.549\n",
      "[74,  2000] loss: 0.550\n",
      "[75,  2000] loss: 0.549\n",
      "[76,  2000] loss: 0.549\n",
      "[77,  2000] loss: 0.549\n",
      "[78,  2000] loss: 0.549\n",
      "[79,  2000] loss: 0.550\n",
      "[80,  2000] loss: 0.550\n",
      "[81,  2000] loss: 0.549\n",
      "[82,  2000] loss: 0.549\n",
      "[83,  2000] loss: 0.550\n",
      "[84,  2000] loss: 0.549\n",
      "[85,  2000] loss: 0.550\n",
      "[86,  2000] loss: 0.549\n",
      "[87,  2000] loss: 0.549\n",
      "[88,  2000] loss: 0.549\n",
      "[89,  2000] loss: 0.549\n",
      "[90,  2000] loss: 0.550\n",
      "[91,  2000] loss: 0.549\n",
      "[92,  2000] loss: 0.549\n",
      "[93,  2000] loss: 0.549\n",
      "[94,  2000] loss: 0.549\n",
      "[95,  2000] loss: 0.550\n",
      "[96,  2000] loss: 0.549\n",
      "[97,  2000] loss: 0.549\n",
      "[98,  2000] loss: 0.549\n",
      "[99,  2000] loss: 0.549\n",
      "[100,  2000] loss: 0.549\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Numpy\n",
    "import numpy as np\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# OS\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "DEVICE = 'cuda:1'\n",
    "SEED = 87\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def print_model(encoder, decoder):\n",
    "    print(\"============== Encoder ==============\")\n",
    "    print(encoder)\n",
    "    print(\"============== Decoder ==============\")\n",
    "    print(decoder)\n",
    "    print(\"\")\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    autoencoder = Autoencoder()\n",
    "    print_model(autoencoder.encoder, autoencoder.decoder)\n",
    "    if torch.cuda.is_available():\n",
    "        autoencoder = autoencoder.to(DEVICE)\n",
    "        print(\"Model moved to GPU in order to speed up training.\")\n",
    "    return autoencoder\n",
    "\n",
    "\n",
    "def get_torch_vars(x):\n",
    "    if torch.cuda.is_available():\n",
    "        x = x.to(DEVICE)\n",
    "    return Variable(x)\n",
    "\n",
    "def imshow(img):\n",
    "    npimg = img.cpu().numpy()\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # Input size: [batch, 3, 32, 32]\n",
    "        # Output size: [batch, 3, 32, 32]\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 12, 4, stride=2, padding=1),            # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(12, 24, 4, stride=2, padding=1),           # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.Conv2d(24, 48, 4, stride=2, padding=1),           # [batch, 48, 4, 4]\n",
    "            nn.ReLU(),\n",
    "# \t\t\tnn.Conv2d(48, 96, 4, stride=2, padding=1),           # [batch, 96, 2, 2]\n",
    "#             nn.ReLU(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(96, 48, 4, stride=2, padding=1),  # [batch, 48, 4, 4]\n",
    "#             nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(48, 24, 4, stride=2, padding=1),  # [batch, 24, 8, 8]\n",
    "            nn.ReLU(),\n",
    "\t\t\tnn.ConvTranspose2d(24, 12, 4, stride=2, padding=1),  # [batch, 12, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(12, 3, 4, stride=2, padding=1),   # [batch, 3, 32, 32]\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "def main():\n",
    "    # Create model\n",
    "    autoencoder = create_model()\n",
    "\n",
    "    # Load data\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToTensor(), ])\n",
    "    trainset = torchvision.datasets.CIFAR10(root='../data/cifar10', train=True,\n",
    "                                            download=True, transform=transform)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=16,\n",
    "                                              shuffle=True, num_workers=2)\n",
    "    testset = torchvision.datasets.CIFAR10(root='../data/cifar10', train=False,\n",
    "                                           download=True, transform=transform)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=16,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    classes = ('plane', 'car', 'bird', 'cat',\n",
    "               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "    # Define an optimizer and criterion\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "\n",
    "    for epoch in range(100):\n",
    "        running_loss = 0.0\n",
    "        for i, (inputs, _) in enumerate(trainloader, 0):\n",
    "            inputs = get_torch_vars(inputs)\n",
    "\n",
    "            # ============ Forward ============\n",
    "            encoded, outputs = autoencoder(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            # ============ Backward ============\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # ============ Logging ============\n",
    "            running_loss += loss.data\n",
    "            if i % 2000 == 1999:\n",
    "                print('[%d, %5d] loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 2000))\n",
    "                running_loss = 0.0\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
