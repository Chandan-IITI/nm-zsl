all:
  task_trainer: "lgm_vae"
  hp:
    base_task_max_num_epochs: 500
    base_task_vae_num_epochs: 300
    max_num_epochs: 100
    vae_num_epochs: 50
    batch_size: 128
    img_target_shape: [224, 224]
    vae_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    clf_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    embedder_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    grad_clipping: {}

    use_class_attrs: false
    model:
      cls_gaussian_dropout_sigma: 0.5
      type: "lgm_vae"
      use_attrs_in_vae: false
      z_dim: 512
      emb_dim: 512
      input_type: "resnet18_feat"
      hid_dim: 512
      cls_hid_dim: 512
      learn_prior: false # Should we train a prior model?

    rehearsal:
      loss_coef: 1.
      batch_size: 128

    kl_term_coef: 0.01

    distillation:
      batch_size: 1024
      enc_loss_coef: 1.
      dec_loss_coef: 1.

    reg_strategy: "ewc"
    synaptic_strength: 0.01
    fisher_prob: 0.5

cub_embeddings:
  data:
    input_type: "resnet18_feat"

  lll_setup:
    num_classes: 200
    num_tasks: 6
    task_sizes: [100, 20, 20, 20, 20, 20]
  hp:
    model:
      num_classes: 200
      use_identity_embedder: true

cub_conv_feats:
  data:
    input_type: "resnet50_feat"

  lll_setup:
    num_classes: 200
    num_tasks: 6
    task_sizes: [100, 20, 20, 20, 20, 20]
  hp:
    model:
      input_type: "resnet50_feat"
      z_dim: 1024
      emb_dim: 1024
      num_classes: 200
      z_spatial_size: 14
      conv_feat_spatial_size: 28
      feat_level: "conv"
      cls_hid_dim: 2048

cub:
  hp:
    num_iters_per_task: 10
    model:
      num_classes: 200

awa:
  hp:
    num_iters_per_task: 5000
    model:
      num_classes: 50