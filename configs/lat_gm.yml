all:
    task_trainer: "lat_gm"
    load_from_checkpoint: "experiments/lat_gm--cub_embedded--42-00019/checkpoints/model-task-0.pt"
    start_task: 1
    hp:
        max_num_epochs: 500
        use_class_attrs: true
        reset_head_before_each_task: false

        model:
            type: "lat_gm"
            z_dim: 256
            hid_dim: 1024
            emb_dim: 512
            use_attrs_in_gen: true
            use_attrs_in_discr: true
            resnet_type: 18
            feat_level: "fc"

        gen_optim:
            type: "sgd"
            # type: "adam"
            # kwargs: {lr: 0.0001, betas: [0.0, 0.9]}
            kwargs: {lr: 0.03}

        discr_optim:
            type: "sgd"
            # kwargs: {lr: 0.0001, betas: [0.0, 0.9]}
            kwargs: {lr: 0.03}

        clf_optim:
            type: "sgd"
            kwargs: {lr: 0.03}

        num_discr_steps_per_gen_step: 5
        loss_coefs:
            gp: 10
            distill: 30.
            discr_cls: 1.
            gen_cls: 0.1

        rehearsal:
            enabled: true
            start_iter: 1000
            batch_size: 100
            loss_coef: 1.

        grad_clipping:
            generator: 100
            discriminator: 100
            embedder: 100

        batch_size: 256
        distill_batch_size: 500

        # Regularizing feature extractor
        synaptic_strength: 10
        fisher_keep_prob: 0.5
        reg_strategy: "ewc"

        # Creativity losses
        creativity:
            enabled: false
            start_iter: -1
            hall_batch_size: 300
            adv_coef: 0.
            entropy_coef: 0.

cub_embedded:
    hp:
        model:
            num_classes: 200
            pretrained: true
            identity_embedder: true

cub:
    hp:
        model:
            num_classes: 200
            pretrained: true

awa:
    hp:
        model:
            num_classes: 50
            pretrained: false
