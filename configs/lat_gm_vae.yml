all:
  task_trainer: "lat_gm_vae"
  hp:
    batch_size: 128
    img_target_shape: [224, 224]
    vae_optim: {kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    clf_optim: {kwargs: {lr: 0.001, betas: [0.0, 0.9]}}

    model:
      type: "lat_gm_vae"
      use_attrs_in_vae: true
      z_dim: 512
      emb_dim: 512
      feat_dim: 512
      hid_dim: 512

      # Should we train a prior model?
      learn_prior_dist: false

    distill_batch_size: 512
    kl_term_coef: 0.01
    enc_distill_loss_coef: 1.
    dec_distill_loss_coef: 1.

    reg_strategy: "ewc"
    synaptic_strength: 0.01
    fisher_prob: 0.5

cub:
  hp:
    num_iters_per_task: 10
    model:
      num_classes: 200
awa:
  hp:
    num_iters_per_task: 5000
    model:
      num_classes: 50