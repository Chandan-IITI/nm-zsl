all:
  task_trainer: "lat_gm_vae"
  hp:
    max_num_epochs: 100
    num_vae_epochs: 50
    batch_size: 128
    img_target_shape: [224, 224]
    vae_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    clf_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    embedder_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    grad_clipping: {}

    use_class_attrs: true
    model:
      type: "lat_gm_vae"
      use_attrs_in_vae: false
      z_dim: 512
      emb_dim: 512
      resnet_type: 18
      hid_dim: 512

      identity_embedder: true
      learn_prior_dist: false # Should we train a prior model?

    rehearsal:
      loss_coef: 5.
      batch_size: 128

    kl_term_coef: 0.01

    distillation:
      batch_size: 1024
      enc_loss_coef: 5.
      dec_loss_coef: 5.

    reg_strategy: "ewc"
    synaptic_strength: 0.01
    fisher_prob: 0.5

cub_embedded:
  data:
    resnet_type: 18

  lll_setup:
    num_classes: 200
    num_tasks: 6
    task_sizes: [100, 20, 20, 20, 20, 20]
  hp:
    model:
      num_classes: 200

cub:
  hp:
    num_iters_per_task: 10
    model:
      num_classes: 200

awa:
  hp:
    num_iters_per_task: 5000
    model:
      num_classes: 50