all:
  task_trainer: "lat_gm_vae"
  hp:
    max_num_epochs: 100
    batch_size: 128
    img_target_shape: [224, 224]
    vae_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    clf_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    embedder_optim: {type: "adam", kwargs: {lr: 0.001, betas: [0.0, 0.9]}}
    grad_clipping: {}

    model:
      type: "lat_gm_vae"
      use_attrs_in_vae: true
      z_dim: 512
      emb_dim: 512
      feat_dim: 512
      hid_dim: 512

      identity_embedder: true
      learn_prior_dist: false # Should we train a prior model?

    distill_batch_size: 512
    kl_term_coef: 0.01
    enc_distill_loss_coef: 1.
    dec_distill_loss_coef: 1.

    reg_strategy: "ewc"
    synaptic_strength: 0.01
    fisher_prob: 0.5

cub_embedded:
  hp:
    model:
      num_classes: 200

cub:
  hp:
    num_iters_per_task: 10
    model:
      num_classes: 200

awa:
  hp:
    num_iters_per_task: 5000
    model:
      num_classes: 50